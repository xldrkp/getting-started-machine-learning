[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning mit Python",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Zusammenfassung",
    "section": "",
    "text": "Zum Einstieg: GPT-3 per Python\nAnalyse: EierML\nWoher bekommt man Daten?"
  },
  {
    "objectID": "further.html",
    "href": "further.html",
    "title": "3  Weiterführende Informationen",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "introduction/index.html",
    "href": "introduction/index.html",
    "title": "Einführung",
    "section": "",
    "text": "Et nostrud eiusmod incididunt ex consequat do amet. Velit ad mollit do voluptate. Proident velit dolor est non Lorem quis excepteur adipisicing cupidatat eu et consectetur irure. Adipisicing proident enim aliqua excepteur sunt ex aliqua cillum laborum qui. Labore irure id velit Lorem aute.\nDo labore quis anim dolore voluptate incididunt est reprehenderit nisi culpa ad nostrud. Ad dolor minim dolor ut magna ullamco pariatur fugiat non ut deserunt in nostrud. Elit tempor aliqua minim fugiat sit cupidatat enim sunt aliquip.\nUt laboris deserunt reprehenderit deserunt dolor eiusmod id nulla commodo consequat eu aliqua adipisicing deserunt. Exercitation pariatur dolor do irure est veniam quis dolore officia minim occaecat excepteur sit. Dolor Lorem mollit culpa ut commodo ullamco qui ex sit in aliquip. Laborum ipsum fugiat aliqua et amet."
  },
  {
    "objectID": "appendices/installation.html",
    "href": "appendices/installation.html",
    "title": "Appendix A — Installation",
    "section": "",
    "text": "Eu dolore proident sint voluptate. Elit consectetur labore culpa duis Lorem elit irure esse ad. Velit laboris velit proident aliquip deserunt nostrud reprehenderit in aute proident tempor non. Incididunt irure adipisicing qui occaecat id laboris commodo aliquip ea ipsum nostrud nulla aute.\nAute consequat commodo eu sit do non in irure ex nostrud nisi exercitation. Voluptate anim ad in excepteur labore dolor irure reprehenderit ipsum nulla aliquip ex nostrud consequat. Labore esse esse enim sit duis reprehenderit ea nostrud cupidatat officia ullamco magna aliquip elit. Aute anim eiusmod minim sunt culpa enim qui sit irure tempor commodo.\nIncididunt occaecat do incididunt ea nostrud sit ipsum nisi dolore eiusmod duis. Nisi velit id anim sunt duis tempor Lorem minim nostrud laborum exercitation. In id et eiusmod cupidatat veniam ex laboris laboris qui. Eu officia voluptate non cillum esse eiusmod nostrud eiusmod. Amet occaecat non est dolor laboris dolore ut laboris adipisicing qui adipisicing. Qui eu minim exercitation sunt laborum nisi qui aute.\nCulpa culpa quis velit laboris aliquip qui tempor fugiat. Ipsum ullamco quis Lorem labore fugiat. Aliqua eu anim ex laborum ad. Proident consequat sint dolor culpa excepteur pariatur dolor tempor laborum sunt laborum voluptate cupidatat laborum. Commodo sit esse voluptate labore eu non ea ut nisi ut sunt. Quis veniam in sint reprehenderit ut sunt occaecat sint."
  },
  {
    "objectID": "introduction/theory.html",
    "href": "introduction/theory.html",
    "title": "1  Statistische Grundlagen",
    "section": "",
    "text": "Die Grundlagen verschiedener statistischer Funktionen und Vorgehensweisen zeige ich entlang der sehr guten Einführung zu Machine Learning auf der Seite der W3School.\nDort stehen die folgenden Konzepte im Mittelpunkt:\nIn diesem Grundlagenskript arbeiten wir zunächst auf das Thema Regression hin, um anschließend noch Konzepte der Klassifizierung kennenzulernen."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projekte",
    "section": "",
    "text": "Reprehenderit est ea labore nulla ullamco aliqua. Do minim magna est aliqua. Nostrud exercitation ea voluptate aliquip mollit non tempor ullamco quis sit tempor ad qui veniam. Et veniam ad enim laboris commodo anim. Lorem sit enim in tempor nisi commodo commodo cillum eiusmod sint ex anim. Dolor duis qui deserunt cupidatat pariatur elit nostrud aliqua ut sint nulla qui sint.\nOfficia Lorem sint voluptate dolore exercitation sit non ea anim minim laboris veniam veniam. Non aliqua eiusmod reprehenderit ut tempor ad consectetur Lorem. Nulla pariatur dolor sit officia veniam aliquip irure laboris. Nulla ipsum nostrud adipisicing labore sit. Dolore dolore pariatur sint minim aliquip veniam cupidatat reprehenderit nostrud. In laboris laboris tempor ex pariatur. Dolore anim ullamco quis duis mollit eiusmod nostrud qui in culpa ut ex.\nCillum reprehenderit proident mollit eu esse laborum laboris qui. Id tempor non adipisicing id veniam voluptate mollit enim culpa dolor eiusmod sint et. Dolore ea elit voluptate pariatur velit mollit magna eu dolor irure. Laborum consequat velit et adipisicing excepteur excepteur magna in tempor est ipsum exercitation irure. Non dolore excepteur nulla ut sint non exercitation ex ad anim voluptate voluptate cupidatat aliqua. Laboris labore minim deserunt anim tempor eiusmod ut. Duis nostrud cupidatat incididunt nostrud in aliquip est exercitation nulla aute sit deserunt incididunt.\nCulpa aliquip ad aliqua ex nulla amet minim. Veniam occaecat aliqua dolor qui ipsum sit culpa in ea est ad. Voluptate eu non culpa ea consectetur culpa et enim non labore consectetur id ad. Pariatur et ipsum nisi amet eu dolore in ex ipsum. Pariatur amet anim incididunt excepteur dolore excepteur esse amet reprehenderit aliquip incididunt mollit."
  },
  {
    "objectID": "projects/eierml.html",
    "href": "projects/eierml.html",
    "title": "13  EierML",
    "section": "",
    "text": "Repository klonen"
  },
  {
    "objectID": "appendices/further.html",
    "href": "appendices/further.html",
    "title": "Appendix B — Weiterführende Informationen",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "introduction/theory.html#section",
    "href": "introduction/theory.html#section",
    "title": "1  Statistische Grundlagen",
    "section": "1.2 ",
    "text": "1.2"
  },
  {
    "objectID": "introduction/theory.html#klassifizierung",
    "href": "introduction/theory.html#klassifizierung",
    "title": "1  Theoretischer Hintergrund",
    "section": "1.1 Klassifizierung",
    "text": "1.1 Klassifizierung\nTempor qui ea fugiat Lorem tempor nostrud quis mollit. Ullamco excepteur eiusmod eu dolor quis esse. Velit occaecat exercitation sint Lorem nulla excepteur elit adipisicing ea. Dolore pariatur exercitation aliquip minim dolor deserunt minim amet dolor Lorem nostrud commodo id fugiat. Eu laborum non eiusmod et. Elit incididunt amet ullamco irure cillum labore nulla excepteur voluptate."
  },
  {
    "objectID": "notebooks/numpy.html",
    "href": "notebooks/numpy.html",
    "title": "6  numpy",
    "section": "",
    "text": "Multidimensionale Arrays und Matrizen, wissenschaftliche Berechnungen"
  },
  {
    "objectID": "introduction/find-data.html",
    "href": "introduction/find-data.html",
    "title": "3  Daten finden",
    "section": "",
    "text": "https://hub.packtpub.com/best-machine-learning-datasets-for-beginners/"
  },
  {
    "objectID": "notebooks/pandas.html",
    "href": "notebooks/pandas.html",
    "title": "7  pandas",
    "section": "",
    "text": "Datenbereinigung und -vorbereitung"
  },
  {
    "objectID": "introduction/theory.html#d",
    "href": "introduction/theory.html#d",
    "title": "1  Theoretischer Hintergrund",
    "section": "1.3 D",
    "text": "1.3 D\nProident tempor non exercitation qui. Qui magna commodo nostrud quis. Id nulla ex laboris elit ullamco ut sunt et cillum ut deserunt. Laboris anim veniam elit aliquip. Dolore anim voluptate eiusmod enim proident ea labore non id ex incididunt tempor dolor sit. Occaecat esse tempor proident elit laboris consequat cupidatat.\nNon nostrud sit nulla officia quis sunt laborum sunt ex deserunt ex quis et in. Sint non elit excepteur aliqua nisi laboris elit velit. Proident eu ipsum tempor voluptate officia ullamco cillum pariatur officia ea nostrud. Pariatur aliqua ullamco aute duis anim.\nTempor amet elit tempor Lorem ipsum aliqua cupidatat. Ullamco nulla nisi velit non elit. Ullamco in ad eiusmod Lorem dolore ut fugiat. Consequat excepteur duis nulla laboris ut dolore officia do ut labore. Magna fugiat aute occaecat duis reprehenderit ipsum aute. Fugiat aliquip aliquip irure irure est aliqua culpa."
  },
  {
    "objectID": "introduction/theory.html#decision",
    "href": "introduction/theory.html#decision",
    "title": "1  Theoretischer Hintergrund",
    "section": "1.3 Decision",
    "text": "1.3 Decision\nProident tempor non exercitation qui. Qui magna commodo nostrud quis. Id nulla ex laboris elit ullamco ut sunt et cillum ut deserunt. Laboris anim veniam elit aliquip. Dolore anim voluptate eiusmod enim proident ea labore non id ex incididunt tempor dolor sit. Occaecat esse tempor proident elit laboris consequat cupidatat.\nNon nostrud sit nulla officia quis sunt laborum sunt ex deserunt ex quis et in. Sint non elit excepteur aliqua nisi laboris elit velit. Proident eu ipsum tempor voluptate officia ullamco cillum pariatur officia ea nostrud. Pariatur aliqua ullamco aute duis anim.\nTempor amet elit tempor Lorem ipsum aliqua cupidatat. Ullamco nulla nisi velit non elit. Ullamco in ad eiusmod Lorem dolore ut fugiat. Consequat excepteur duis nulla laboris ut dolore officia do ut labore. Magna fugiat aute occaecat duis reprehenderit ipsum aute. Fugiat aliquip aliquip irure irure est aliqua culpa."
  },
  {
    "objectID": "introduction/theory.html#decision-tree",
    "href": "introduction/theory.html#decision-tree",
    "title": "1  Statistische Grundlagen",
    "section": "1.1 Decision tree",
    "text": "1.1 Decision tree\n\nhttps://www.askpython.com/python/machine-learning-introduction\n\nProident tempor non exercitation qui. Qui magna commodo nostrud quis. Id nulla ex laboris elit ullamco ut sunt et cillum ut deserunt. Laboris anim veniam elit aliquip. Dolore anim voluptate eiusmod enim proident ea labore non id ex incididunt tempor dolor sit. Occaecat esse tempor proident elit laboris consequat cupidatat."
  },
  {
    "objectID": "introduction/theory.html#lineare-regression",
    "href": "introduction/theory.html#lineare-regression",
    "title": "1  Theoretischer Hintergrund",
    "section": "1.1 Lineare Regression",
    "text": "1.1 Lineare Regression\nDie Grundlagen verschiedener statistischer Funktionen und Vorgehensweisen zeige ich mithilfe der sehr guten Einführung zu Machine Learning auf der Seite der W3School"
  },
  {
    "objectID": "introduction/pretrained-models.html",
    "href": "introduction/pretrained-models.html",
    "title": "4  Vortrainierte Modelle",
    "section": "",
    "text": "https://www.tensorflow.org/js/models\nhttps://firebase.google.com/docs/ml-kit\nhttps://tfhub.dev/"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Velit in voluptate irure consequat anim velit voluptate mollit duis aute velit. Proident ad id quis minim proident mollit eu elit reprehenderit dolore fugiat. Incididunt adipisicing pariatur deserunt ullamco elit qui nostrud qui aute. Exercitation amet est labore minim ad Lorem anim magna aute consequat aliquip adipisicing aliquip commodo. Aute quis laboris culpa nulla culpa cillum cillum pariatur fugiat consectetur sint velit."
  },
  {
    "objectID": "notebooks/matplotlib.html",
    "href": "notebooks/matplotlib.html",
    "title": "10  matplotlib",
    "section": "",
    "text": "https://matplotlib.org/stable/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py"
  },
  {
    "objectID": "notebooks/numpy.html#section",
    "href": "notebooks/numpy.html#section",
    "title": "5  numpy",
    "section": "5.1 ",
    "text": "5.1 \n\n#!pip install numpy\n\n\nimport numpy as np"
  },
  {
    "objectID": "notebooks/numpy.html#einfache-statistische-funktionen",
    "href": "notebooks/numpy.html#einfache-statistische-funktionen",
    "title": "6  numpy",
    "section": "6.1 Einfache statistische Funktionen",
    "text": "6.1 Einfache statistische Funktionen\nhttps://www.w3schools.com/python/python_ml_mean_median_mode.asp\n\n### Mean\n\n\n#!pip install numpy\n\n\nspeed = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n\n\n6.1.1 Mean (Mittelwert)\n\nimport numpy as np\n\n\nx = np.median(speed)\nx\n\n87.0"
  },
  {
    "objectID": "notebooks/numpy.html#übungen-zum-auswählen-von-daten-aus-einer-tabelle",
    "href": "notebooks/numpy.html#übungen-zum-auswählen-von-daten-aus-einer-tabelle",
    "title": "6  numpy",
    "section": "6.2 Übungen zum Auswählen von Daten aus einer Tabelle",
    "text": "6.2 Übungen zum Auswählen von Daten aus einer Tabelle\nHier geht es vornehmlich um die Methode iloc in numpy und pandas.\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html?highlight=iloc#pandas.DataFrame.iloc"
  },
  {
    "objectID": "notebooks/ml-handson.html",
    "href": "notebooks/ml-handson.html",
    "title": "5  Grundlagen Machine Learning",
    "section": "",
    "text": "#!pip install numpy\n#!pip install pandas\n#!pip install matplotlib"
  },
  {
    "objectID": "notebooks/ml-handson.html#bibliotheken-importieren",
    "href": "notebooks/ml-handson.html#bibliotheken-importieren",
    "title": "5  Grundlagen Machine Learning",
    "section": "5.2 Bibliotheken importieren",
    "text": "5.2 Bibliotheken importieren\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/ml-handson.html#daten-importieren",
    "href": "notebooks/ml-handson.html#daten-importieren",
    "title": "5  Grundlagen Machine Learning",
    "section": "5.3 Daten importieren",
    "text": "5.3 Daten importieren\nFeatures sind hier die unabhängigen Variablen, mit denen die abhängige Variable vorausgesagt werden kann. Diese abhängige Variable steht in der Regel in der letzten Spalte der Daten.\n\ndataset = pd.read_csv(\"data/dataset.csv\")\n# Merke: Die Obergrenze von *ranges* in Python ist ausgeschlossen! Daher wird die letzte Spalte mit -1 abgezogen.\n# Aufgabe: Recherchiere in der Dokumentation von pandas das Konzept von iloc.\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nprint(X)\n\n[['Frankreich' 44.0 72000.0]\n ['Griechenland' 27.0 48000.0]\n ['Deutschland' 30.0 54000.0]\n ['Griechenland' 38.0 61000.0]\n ['Deutschland' 40.0 nan]\n ['Frankreich' 35.0 58000.0]\n ['Griechenland' nan 52000.0]\n ['Frankreich' 48.0 79000.0]\n ['Deutschland' 50.0 83000.0]\n ['Frankreich' 37.0 67000.0]]\n\n\n\nprint(y)\n\n['Nein' 'Ja' 'Nein' 'Nein' 'Ja' 'Ja' 'Nein' 'Ja' 'Nein' 'Ja']"
  },
  {
    "objectID": "notebooks/ml-handson.html#fehlende-daten-berücksichtigen",
    "href": "notebooks/ml-handson.html#fehlende-daten-berücksichtigen",
    "title": "5  Grundlagen Machine Learning",
    "section": "5.4 Fehlende Daten berücksichtigen",
    "text": "5.4 Fehlende Daten berücksichtigen\nWenn wenige Daten fehlen, können diese Datensätze in der Regel ignoriert bzw. entfernt werden. Fehlen in vielen Datensätzen Daten, gibt es verschiedene Strategien, diese zu ersetzen. Eine davon ist, den Mittelwert aus den vorhandenen Daten zu bilden.\n\n# Aufgabe: Recherchiere in der Dokumentation von scikit-learn zu den im Folgenden verwendeten Funktionen!\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:, 1:3])\nX[:, 1:3] = imputer.transform(X[:, 1:3])\n\n\nprint(X)\n\n[['Frankreich' 44.0 72000.0]\n ['Griechenland' 27.0 48000.0]\n ['Deutschland' 30.0 54000.0]\n ['Griechenland' 38.0 61000.0]\n ['Deutschland' 40.0 63777.77777777778]\n ['Frankreich' 35.0 58000.0]\n ['Griechenland' 38.77777777777778 52000.0]\n ['Frankreich' 48.0 79000.0]\n ['Deutschland' 50.0 83000.0]\n ['Frankreich' 37.0 67000.0]]"
  },
  {
    "objectID": "notebooks/ml-handson.html#daten-aufteilen-in-training--und-testset",
    "href": "notebooks/ml-handson.html#daten-aufteilen-in-training--und-testset",
    "title": "5  Grundlagen Machine Learning",
    "section": "5.6 Daten aufteilen in Training- und Testset",
    "text": "5.6 Daten aufteilen in Training- und Testset\nFür den eigentlichen Prozess des “Maschinenlernens” wird das Datenset in ein Trainingsset und ein Testset aufgeteilt.\nMit dem Trainingsset wird das Machine-Learning-Modell trainiert, mit dem Testset wird die Performance des Modells getestet. Hierbei wird so getan, als ob es sich bei den Testdaten um zukünftige reale Daten handelt.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\n\nprint(X_train)\n\n[[0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 40.0 63777.77777777778]\n [0.0 1.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 48.0 79000.0]\n [1.0 0.0 0.0 50.0 83000.0]\n [0.0 1.0 0.0 35.0 58000.0]]\n\n\n\nprint(X_test)\n\n[[1.0 0.0 0.0 30.0 54000.0]\n [0.0 1.0 0.0 37.0 67000.0]]\n\n\n\nprint(y_train)\n\n[1 0 1 1 0 0 1 0]\n\n\n\nprint(y_test)\n\n[1 0]"
  },
  {
    "objectID": "notebooks/ml-handson.html#feature-scaling",
    "href": "notebooks/ml-handson.html#feature-scaling",
    "title": "5  Grundlagen Machine Learning",
    "section": "5.7 Feature Scaling",
    "text": "5.7 Feature Scaling\nFeature Scaling ist die Anpassung der unabhängigen Daten in einer Weise, dass sie alle im gleichen Verhältnis zueinander stehen. Dadurch wird vermieden, dass ein Feature dominiert.\nWichtig: Feature Scaling wird immer erst nach der Aufteilung des Datensets durchgeführt! Damit wird Informationsverlust bei den Testdaten vermieden.\n\n5.7.1 Normalisierung\nHier werden alle Werte in den Bereich zwischen 0 und 1 eingeordnet. Normalisierung wird angewendet, wenn es eine normale Verteilung in den meisten Features gibt.\n\n\n5.7.2 Standardisierung\nHierbei werden alle Werte in den Bereich von -3 und +3 eingeordnet. Standardisierung funktioniert immer, egal wie verschieden die Verteilung der Werte in den Features ist.\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = sc.transform(X_test[:, 3:])\n\n\nprint(X_train)\n\n[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n [1.0 0.0 0.0 -0.014117293757057777 -0.07013167641635372]\n [0.0 1.0 0.0 0.566708506533324 0.633562432710455]\n [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n [0.0 1.0 0.0 1.1475343068237058 1.232653363453549]\n [1.0 0.0 0.0 1.4379472069688968 1.5749910381638885]\n [0.0 1.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n\n\n\nprint(X_test)\n\n[[1.0 0.0 0.0 -1.4661817944830124 -0.9069571034860727]\n [0.0 1.0 0.0 -0.44973664397484414 0.2056403393225306]]"
  },
  {
    "objectID": "notebooks/ml-handson.html#kategorien-in-den-daten-kodieren",
    "href": "notebooks/ml-handson.html#kategorien-in-den-daten-kodieren",
    "title": "5  Grundlagen Machine Learning",
    "section": "5.5 Kategorien in den Daten kodieren",
    "text": "5.5 Kategorien in den Daten kodieren\nDamit der Algorithmus die Daten schnell und eindeutig verarbeiten kann, müssen Textwerte als Zahlenwerte kodiert werden. Ein gängiges Verfahren ist hierbei, binäre Vektoren aus den möglichen Kombinationen der Kategorien zu erstellen. Bei diesem Vorgehen werden neue Spalten angelegt, die diese Vektoren abbilden. Lies hierzu auch den Artikel über One Hot Encoding, wie dieses Verfahren genannt wird.\nDa im vorliegenden Datenset sowohl in den unabhängigen als auch in der abhängigen Variable kategorische Daten vorliegen, ist der Vorgang zweimal notwendig.\n\n5.5.1 Die unabhängige Variable kodieren\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\n\n\nprint(X)\n\n[[0.0 1.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [1.0 0.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [1.0 0.0 0.0 40.0 63777.77777777778]\n [0.0 1.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [0.0 1.0 0.0 48.0 79000.0]\n [1.0 0.0 0.0 50.0 83000.0]\n [0.0 1.0 0.0 37.0 67000.0]]\n\n\n\n\n5.5.2 Die abhängige Variable kodieren\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n\nprint(y)\n\n[1 0 1 1 0 0 1 0 1 0]"
  },
  {
    "objectID": "notebooks/ml-handson.html#bibliotheken-installieren-1",
    "href": "notebooks/ml-handson.html#bibliotheken-installieren-1",
    "title": "5  Grundlagen Machine Learning",
    "section": "5.6 Bibliotheken installieren",
    "text": "5.6 Bibliotheken installieren"
  },
  {
    "objectID": "notebooks/pandas.html#normalisierung-vs.-standardisierung",
    "href": "notebooks/pandas.html#normalisierung-vs.-standardisierung",
    "title": "7  pandas",
    "section": "7.1 Normalisierung vs. Standardisierung",
    "text": "7.1 Normalisierung vs. Standardisierung\nhttps://www.statology.org/standardization-vs-normalization/"
  },
  {
    "objectID": "notebooks/preprocessing.html",
    "href": "notebooks/preprocessing.html",
    "title": "8  Daten vorbereiten (data preprocessing)",
    "section": "",
    "text": "#!pip install numpy\n#!pip install pandas\n#!pip install matplotlib"
  },
  {
    "objectID": "notebooks/preprocessing.html#bibliotheken-importieren",
    "href": "notebooks/preprocessing.html#bibliotheken-importieren",
    "title": "8  Daten vorbereiten (data preprocessing)",
    "section": "8.2 Bibliotheken importieren",
    "text": "8.2 Bibliotheken importieren\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/preprocessing.html#daten-importieren",
    "href": "notebooks/preprocessing.html#daten-importieren",
    "title": "8  Daten vorbereiten (data preprocessing)",
    "section": "8.3 Daten importieren",
    "text": "8.3 Daten importieren\nFeatures sind hier die unabhängigen Variablen, mit denen die abhängige Variable vorausgesagt werden kann. Diese abhängige Variable steht in der Regel in der letzten Spalte der Daten.\n\ndataset = pd.read_csv(\"data/dataset.csv\")\n# Merke: Die Obergrenze von *ranges* in Python ist ausgeschlossen! Daher wird die letzte Spalte mit -1 abgezogen.\n# Aufgabe: Recherchiere in der Dokumentation von pandas das Konzept von iloc.\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nprint(X)\n\n\nprint(y)"
  },
  {
    "objectID": "notebooks/preprocessing.html#fehlende-daten-berücksichtigen",
    "href": "notebooks/preprocessing.html#fehlende-daten-berücksichtigen",
    "title": "8  Daten vorbereiten (data preprocessing)",
    "section": "8.4 Fehlende Daten berücksichtigen",
    "text": "8.4 Fehlende Daten berücksichtigen\nWenn wenige Daten fehlen, können diese Datensätze in der Regel ignoriert bzw. entfernt werden. Fehlen in vielen Datensätzen Daten, gibt es verschiedene Strategien, diese zu ersetzen. Eine davon ist, den Mittelwert aus den vorhandenen Daten zu bilden.\n\n# Aufgabe: Recherchiere in der Dokumentation von scikit-learn zu den im Folgenden verwendeten Funktionen!\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:, 1:3])\nX[:, 1:3] = imputer.transform(X[:, 1:3])\n\n\nprint(X)"
  },
  {
    "objectID": "notebooks/preprocessing.html#kategorien-in-den-daten-kodieren",
    "href": "notebooks/preprocessing.html#kategorien-in-den-daten-kodieren",
    "title": "8  Daten vorbereiten (data preprocessing)",
    "section": "8.5 Kategorien in den Daten kodieren",
    "text": "8.5 Kategorien in den Daten kodieren\nDamit der Algorithmus die Daten schnell und eindeutig verarbeiten kann, müssen Textwerte als Zahlenwerte kodiert werden. Ein gängiges Verfahren ist hierbei, binäre Vektoren aus den möglichen Kombinationen der Kategorien zu erstellen. Bei diesem Vorgehen werden neue Spalten angelegt, die diese Vektoren abbilden. Lies hierzu auch den Artikel über One Hot Encoding, wie dieses Verfahren genannt wird.\nDa im vorliegenden Datenset sowohl in den unabhängigen als auch in der abhängigen Variable kategorische Daten vorliegen, ist der Vorgang zweimal notwendig.\n\n8.5.1 Die unabhängige Variable kodieren\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\n\n\nprint(X)\n\n\n\n8.5.2 Die abhängige Variable kodieren\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n\nprint(y)"
  },
  {
    "objectID": "notebooks/preprocessing.html#daten-aufteilen-in-training--und-testset",
    "href": "notebooks/preprocessing.html#daten-aufteilen-in-training--und-testset",
    "title": "8  Daten vorbereiten (data preprocessing)",
    "section": "8.6 Daten aufteilen in Training- und Testset",
    "text": "8.6 Daten aufteilen in Training- und Testset\nFür den eigentlichen Prozess des “Maschinenlernens” wird das Datenset in ein Trainingsset und ein Testset aufgeteilt.\nMit dem Trainingsset wird das Machine-Learning-Modell trainiert, mit dem Testset wird die Performance des Modells getestet. Hierbei wird so getan, als ob es sich bei den Testdaten um zukünftige reale Daten handelt.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\n\nprint(X_train)\n\n\nprint(X_test)\n\n\nprint(y_train)\n\n\nprint(y_test)"
  },
  {
    "objectID": "notebooks/preprocessing.html#feature-scaling",
    "href": "notebooks/preprocessing.html#feature-scaling",
    "title": "8  Daten vorbereiten (data preprocessing)",
    "section": "8.7 Feature Scaling",
    "text": "8.7 Feature Scaling\nFeature Scaling ist die Anpassung der unabhängigen Daten in einer Weise, dass sie alle im gleichen Verhältnis zueinander stehen. Dadurch wird vermieden, dass ein Feature dominiert.\nWichtig: Feature Scaling wird immer erst nach der Aufteilung des Datensets durchgeführt! Damit wird Informationsverlust bei den Testdaten vermieden.\n\n8.7.1 Normalisierung\nHier werden alle Werte in den Bereich zwischen 0 und 1 eingeordnet. Normalisierung wird angewendet, wenn es eine normale Verteilung in den meisten Features gibt.\n\n\n8.7.2 Standardisierung\nHierbei werden alle Werte in den Bereich von -3 und +3 eingeordnet. Standardisierung funktioniert immer, egal wie verschieden die Verteilung der Werte in den Features ist.\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = sc.transform(X_test[:, 3:])\n\n\nprint(X_train)\n\n\nprint(X_test)"
  },
  {
    "objectID": "notebooks/linear-regression.html",
    "href": "notebooks/linear-regression.html",
    "title": "9  Einfache lineare Regression",
    "section": "",
    "text": "Bei der einfachen Linearen Regression geht es darum, einen Wert in Abhängigkeit von anderen Werten vorherzusagen.\nBspw. kann die Frage gestellt werden, die Erfahrung und Gehalt zusammenhängen.\nDie Formel, mit der sich eine lineare Regression beschreiben lässt, ist die folgende:\n\\[\n  y = b_0 + b_1x\n\\]\nBei der Linearen Regression geht es darum, die beste Linie durch die Daten zu finden, um dann auf Basis dieser Linie neue Werte vorhersagen zu können.\nMit einem Algorithmus diese Linie zu ermitteln bedeutet, zunächst die Summe der Quadrate aller Abweichungen von der provisorischen Linie zu bilden und anschließend die Linie zu verwenden, die die kleineste Summe der Quadrate liefert."
  },
  {
    "objectID": "notebooks/linear-regression.html#beispiel-gehalt-bestimmen-in-abhängigkeit-von-der-berufserfahrung",
    "href": "notebooks/linear-regression.html#beispiel-gehalt-bestimmen-in-abhängigkeit-von-der-berufserfahrung",
    "title": "9  Einfache lineare Regression",
    "section": "9.1 Beispiel: Gehalt bestimmen in Abhängigkeit von der Berufserfahrung",
    "text": "9.1 Beispiel: Gehalt bestimmen in Abhängigkeit von der Berufserfahrung\n\n9.1.1 Bibliotheken importieren\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n9.1.2 Daten importieren\n\ndataset = pd.read_csv(\"data/gehalt.csv\")\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\n\n9.1.3 Daten aufteilen\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n\n\n9.1.4 Trainieren des Modells für Lineare Regression\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n9.1.5 Ergebnisse aus dem Testset vorhersagen\n\ny_pred = regressor.predict(X_test)\n\n\n\n9.1.6 Trainingergebnisse visualisieren\n\nplt.scatter(X_train, y_train, color = 'red')\nplt.plot(X_train, regressor.predict(X_train), color = 'blue')\nplt.title('Gehalt vs. Erfahrung (Trainingset)')\nplt.xlabel('Berufserfahrung in Jahren')\nplt.ylabel('Gehalt')\nplt.show()\n\n\n\n\n\n\n9.1.7 Testergebnisse visualisieren\n\nplt.scatter(X_test, y_test, color = 'red')\n# Hier muss nichts ersetzt werden!\nplt.plot(X_train, regressor.predict(X_train), color = 'blue')\nplt.title('Gehalt vs. Erfahrung (Testset)')\nplt.xlabel('Berufserfahrung in Jahren')\nplt.ylabel('Gehalt')\nplt.show()"
  },
  {
    "objectID": "vorbereitung/numpy.html",
    "href": "vorbereitung/numpy.html",
    "title": "6  numpy",
    "section": "",
    "text": "Multidimensionale Arrays und Matrizen, wissenschaftliche Berechnungen"
  },
  {
    "objectID": "vorbereitung/numpy.html#einfache-statistische-funktionen",
    "href": "vorbereitung/numpy.html#einfache-statistische-funktionen",
    "title": "6  numpy",
    "section": "6.1 Einfache statistische Funktionen",
    "text": "6.1 Einfache statistische Funktionen\nhttps://www.w3schools.com/python/python_ml_mean_median_mode.asp\n\n### Mean\n\n\n#!pip install numpy\n\n\nspeed = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n\n\n6.1.1 Mean (Mittelwert)\n\nimport numpy as np\n\n\nx = np.median(speed)\nx\n\n87.0"
  },
  {
    "objectID": "vorbereitung/numpy.html#übungen-zum-auswählen-von-daten-aus-einer-tabelle",
    "href": "vorbereitung/numpy.html#übungen-zum-auswählen-von-daten-aus-einer-tabelle",
    "title": "6  numpy",
    "section": "6.2 Übungen zum Auswählen von Daten aus einer Tabelle",
    "text": "6.2 Übungen zum Auswählen von Daten aus einer Tabelle\nHier geht es vornehmlich um die Methode iloc in numpy und pandas.\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html?highlight=iloc#pandas.DataFrame.iloc"
  },
  {
    "objectID": "vorbereitung/pandas.html",
    "href": "vorbereitung/pandas.html",
    "title": "6  pandas",
    "section": "",
    "text": "Datenbereinigung und -vorbereitung"
  },
  {
    "objectID": "vorbereitung/pandas.html#normalisierung-vs.-standardisierung",
    "href": "vorbereitung/pandas.html#normalisierung-vs.-standardisierung",
    "title": "6  pandas",
    "section": "6.1 Normalisierung vs. Standardisierung",
    "text": "6.1 Normalisierung vs. Standardisierung\nhttps://www.statology.org/standardization-vs-normalization/"
  },
  {
    "objectID": "vorbereitung/matplotlib.html",
    "href": "vorbereitung/matplotlib.html",
    "title": "5  matplotlib",
    "section": "",
    "text": "https://matplotlib.org/stable/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py"
  },
  {
    "objectID": "notebooks/linear-regression.html#eine-einfache-vorhersage-machen",
    "href": "notebooks/linear-regression.html#eine-einfache-vorhersage-machen",
    "title": "9  Einfache lineare Regression",
    "section": "9.2 Eine einfache Vorhersage machen",
    "text": "9.2 Eine einfache Vorhersage machen\n\nprint(regressor.predict([[12]]))\n\n[138531.00067138]"
  },
  {
    "objectID": "notebooks/linear-regression.html#die-finale-gleichung-der-regression-erhalten",
    "href": "notebooks/linear-regression.html#die-finale-gleichung-der-regression-erhalten",
    "title": "9  Einfache lineare Regression",
    "section": "9.3 Die finale Gleichung der Regression erhalten",
    "text": "9.3 Die finale Gleichung der Regression erhalten\n\nprint(regressor.coef_)\n\n[9312.57512673]\n\n\n\nprint(regressor.intercept_)\n\n26780.09915062818\n\n\nDie finale Gleichung mit diesen Werten lautet daher:\n\\[\nGehalt = 9312,58 x JahreErfahrung + 26780,1\n\\]"
  },
  {
    "objectID": "introduction/theory.html#literatur-zur-einführung",
    "href": "introduction/theory.html#literatur-zur-einführung",
    "title": "1  Statistische Grundlagen",
    "section": "1.2 Literatur zur Einführung",
    "text": "1.2 Literatur zur Einführung\n\nKirste und Schürholz (2019)\nStubbe, Wessels, und Zinke (2019)\n\n\n\n\n\nKirste, Moritz, und Markus Schürholz. 2019. „Einleitung: Entwicklungswege zur KI“. In Künstliche Intelligenz, herausgegeben von Volker Wittpahl, 21–35. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-58042-4_1.\n\n\nStubbe, Julian, Jan Wessels, und Guido Zinke. 2019. „Neue Intelligenz, neue Ethik?“ In Künstliche Intelligenz, herausgegeben von Volker Wittpahl, 239–54. Berlin, Heidelberg: Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-58042-4_15."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Quellen",
    "section": "",
    "text": "Kirste, Moritz, and Markus Schürholz. 2019. “Einleitung:\nEntwicklungswege zur KI.” In Künstliche Intelligenz,\nedited by Volker Wittpahl, 21–35. Berlin, Heidelberg:\nSpringer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-58042-4_1.\n\n\nStubbe, Julian, Jan Wessels, and Guido Zinke. 2019. “Neue\nIntelligenz, neue Ethik?” In Künstliche Intelligenz,\nedited by Volker Wittpahl, 239–54. Berlin, Heidelberg:\nSpringer Berlin Heidelberg. https://doi.org/10.1007/978-3-662-58042-4_15."
  }
]